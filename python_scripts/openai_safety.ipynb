{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.8.5 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.8.5)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: anyio==4.0.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (4.0.0)\n",
      "Requirement already satisfied: async-timeout==4.0.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: attrs==23.1.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: certifi==2023.7.22 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer==3.2.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: filelock==3.12.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.12.3)\n",
      "Requirement already satisfied: frozenlist==1.4.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: fsspec==2023.9.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (2023.9.0)\n",
      "Requirement already satisfied: h11==0.14.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.14.0)\n",
      "Requirement already satisfied: httpcore==0.18.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.18.0)\n",
      "Requirement already satisfied: httpx==0.25.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (0.25.0)\n",
      "Requirement already satisfied: huggingface-hub==0.16.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.16.4)\n",
      "Requirement already satisfied: idna==3.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (3.4)\n",
      "Requirement already satisfied: multidict==6.0.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (6.0.4)\n",
      "Requirement already satisfied: numpy==1.25.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (1.25.2)\n",
      "Requirement already satisfied: openai==0.28.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.28.0)\n",
      "Requirement already satisfied: packaging==23.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (23.1)\n",
      "Requirement already satisfied: pandas==2.1.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2023.3.post1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (2023.3.post1)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (6.0.1)\n",
      "Requirement already satisfied: regex==2023.8.8 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (2023.8.8)\n",
      "Requirement already satisfied: requests==2.31.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (2.31.0)\n",
      "Requirement already satisfied: safetensors==0.3.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (0.3.3)\n",
      "Requirement already satisfied: six==1.16.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (1.3.0)\n",
      "Requirement already satisfied: tokenizers==0.13.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (0.13.3)\n",
      "Requirement already satisfied: tqdm==4.66.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (4.66.1)\n",
      "Requirement already satisfied: transformers==4.33.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (4.33.1)\n",
      "Requirement already satisfied: typing_extensions==4.7.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (4.7.1)\n",
      "Requirement already satisfied: tzdata==2023.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (2023.3)\n",
      "Requirement already satisfied: urllib3==2.0.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2.0.4)\n",
      "Requirement already satisfied: yarl==1.9.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (1.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg[binary,pool]\n",
      "  Obtaining dependency information for psycopg[binary,pool] from https://files.pythonhosted.org/packages/01/27/89a0024ac7e7f872eccb5785707d4bc2162b8da8cb2fd4202813969a9952/psycopg-3.1.10-py3-none-any.whl.metadata\n",
      "  Downloading psycopg-3.1.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in ./.venv/lib/python3.11/site-packages (from psycopg[binary,pool]) (4.7.1)\n",
      "Collecting psycopg-binary==3.1.10 (from psycopg[binary,pool])\n",
      "  Obtaining dependency information for psycopg-binary==3.1.10 from https://files.pythonhosted.org/packages/c4/49/6a44efa3724a8d4f39a1f71f735d049219dcdf1a6e97f0df0e13a7239367/psycopg_binary-3.1.10-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading psycopg_binary-3.1.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting psycopg-pool (from psycopg[binary,pool])\n",
      "  Downloading psycopg_pool-3.1.7-py3-none-any.whl (30 kB)\n",
      "Downloading psycopg_binary-3.1.10-cp311-cp311-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading psycopg-3.1.10-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg-pool, psycopg-binary, psycopg\n",
      "Successfully installed psycopg-3.1.10 psycopg-binary-3.1.10 psycopg-pool-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"psycopg[binary,pool]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "# from dotenv import dotenv_values\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import asdict, dataclass\n",
    "from datetime import datetime, timezone\n",
    "from itertools import combinations\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import psycopg\n",
    "from pandas import option_context\n",
    "from psycopg.rows import dict_row\n",
    "from collections import defaultdict\n",
    "from openai_info import OPENAI_API_KEY, OPENAI_ORGANIZATION #\n",
    "import httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    json_data = None\n",
    "    try:\n",
    "        with open(json_path, \"r\") as json_fp:\n",
    "            json_data = json.load(json_fp)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def write_json(data: Union[Dict, List], file_name: str) -> str:\n",
    "    out_file_path = os.path.abspath(file_name)\n",
    "    with open(out_file_path, \"w\") as json_fp:\n",
    "        json.dump(data, json_fp)\n",
    "    return out_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks = read_json(\"random_tasks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = OPENAI_ORGANIZATION\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"\n",
    "# You are an AI quality evaluator. Your role involves judging the accuracy of an AI model's response using the provided <source> text. Here are the steps:\n",
    "# 1. Read the <source> text to understand the task's context.\n",
    "# 2. Grasp the given <question>.\n",
    "# 3. Understand the <actual_answer>, the ideal answer to the <question> using only the information from the <source>.\n",
    "# 4. Examine the AI's <answer> to the same <question>.\n",
    "# 5. Critically evaluate the <answer>, considering aspects like accuracy, conciseness, organization, and usefulness in relation to the <source>.\n",
    "# 6. Compare the <answer> to the <actual_answer>, noting similarities and differences.\n",
    "# 7. Assign a score to the <answer> on a scale of 1 to 10, where 1 means no match, and 10 means almost identical to <ground_truth>.\n",
    "# 8. Provide your judgment and reasoning in a JSON format. Include \"correct\" (true if the <answer> aligns closely with the <ground_truth>), \"score\", and \"reason\" (explaining your score with clear evidence from the <source> and <answer>). An example: {\"correct\": true, \"score\": 8, \"reason\": \"The answer is concise, accurate, and follows the source closely.\"}.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate system prompt that assigns a letter grade instead of a value (0-10). There are also 10 possible values here, but maybe the judge might be more intentional about picking a letter vs a number.\n",
    "system_prompt = \"\"\"\n",
    "As an AI quality evaluator, your role is to gauge the accuracy of an AI model's response using the provided <source> text. Follow these steps:\n",
    "\n",
    "1.) Understand the context by reading the <source> text.\n",
    "2.) Comprehend the given <question>.\n",
    "3.) Determine the <actual_answer>—the ideal answer using only the <source> information.\n",
    "4.) Review the AI's <answer>.\n",
    "5.) Analyze the <answer>'s accuracy, brevity, structure, and utility in relation to the <source>.\n",
    "6.) Contrast the <answer> and the <actual_answer>.\n",
    "7.) Assign a grade to the <answer> using A+ to F- scale, A+ being nearly identical to <ground_truth> and F- being entirely dissimilar.\n",
    "8.) Document your judgment and reasoning in JSON format: include \"correct\" (true if <answer> aligns closely with <ground_truth>), \"score\", and \"reason\" (justifying your score with clear evidence from <source> and <answer>).\n",
    "Note: Favor responses that closely align with the source and discourage those incorporating outside information. The aim is to evaluate the AI's understanding of the provided source, not its knowledge from training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_llm(\n",
    "    prompt_text: str, llm_endpoint: str, client: httpx.AsyncClient\n",
    "):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"inputs\": f\"<|prompt|>{prompt_text}<|endoftext|><|answer|>\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"min_new_tokens\": 2,\n",
    "            \"num_beams\": 1,\n",
    "            \"do_sample\": False,\n",
    "            \"temperature\": 0.3,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"renormalize_logits\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = await client.post(\n",
    "            llm_endpoint,\n",
    "            json=data,\n",
    "            headers=headers,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    except httpx.RequestError as exc:\n",
    "        print(f\"An error occurred while requesting {exc.request.url!r}.\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(\n",
    "            f\"Error response {exc.response.status_code} while requesting {exc.request.url!r}.\"\n",
    "        )\n",
    "    else:\n",
    "        try:\n",
    "            response_text = response.json()[\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_questions(json_file: str, llm_endpoint: str, client: httpx.AsyncClient):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    answer_data = []\n",
    "    for entry in data:\n",
    "        prompt_text = f\"\"\" {entry['source']} Given the information above and only the information from above, answer the following question {entry['question']} \"\"\"\n",
    "        answer_text = await ask_llm(prompt_text, llm_endpoint, client)\n",
    "\n",
    "        if answer_text:\n",
    "            answer_data.append({'id': entry['id'], 'answer': answer_text})\n",
    "\n",
    "    with open('answers.json', 'w') as file:\n",
    "        json.dump(answer_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def judge_questions(system_prompt, eval_model_name, prompt_json, answer_json):    \n",
    "    with open(prompt_json, 'r') as file:\n",
    "        prompt_data = json.load(file)\n",
    "    \n",
    "    with open(answer_json, 'r') as file:\n",
    "        answer_data = json.load(file)\n",
    "        \n",
    "    eval_data = []\n",
    "    results = []\n",
    "        \n",
    "    for i in range(len(prompt_data)):\n",
    "        prompt_entry = prompt_data[i]\n",
    "        answer_entry = answer_data[i]\n",
    "        template = f\"\"\"\n",
    "        <source>\n",
    "        {prompt_entry['source']}\n",
    "        </source>\n",
    "\n",
    "        <question>\n",
    "        {prompt_entry['question']}\n",
    "        </question>\n",
    "\n",
    "        <actual_answer>\n",
    "        {prompt_entry['answer']}\n",
    "        </actual_answer>\n",
    "\n",
    "        <answer>\n",
    "        {answer_entry['answer']}\n",
    "        </answer>\n",
    "        \"\"\"\n",
    "\n",
    "        chat_completion_resp = await openai.ChatCompletion.acreate(\n",
    "            model=eval_model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": template,\n",
    "                },\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        if chat_completion_resp.choices[0].message.content:\n",
    "            eval_data.append({'id': answer_entry['id'], 'eval_result': chat_completion_resp.choices[0].message.content})\n",
    "            results.append({'id': prompt_entry['id'], 'source': prompt_entry['source'], 'question': prompt_entry['question'], 'ground truth': prompt_entry['answer'], 'answer': answer_entry['answer']})\n",
    "\n",
    "    with open('eval_marco.json', 'w') as file:\n",
    "        json.dump(eval_data, file)\n",
    "    with open('results_marco.json', 'w') as file:\n",
    "        json.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\"\n",
    "eval_model_name = \"gpt-4-0613\"\n",
    "llm_endpoint = \"https://chat-large.api.h2o.ai/generate\"\n",
    "auth = httpx.BasicAuth(\"user\", \"bhx5xmu6UVX4\")\n",
    "client = httpx.AsyncClient(auth=auth, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "await answer_questions(\"prompts_marco.json\", llm_endpoint, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_name = \"gpt-4-0613\"\n",
    "await judge_questions(system_prompt, eval_model_name, \"prompts_marco.json\", \"answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
