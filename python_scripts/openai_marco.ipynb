{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.8.5 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.8.5)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Collecting anyio==4.0.0 (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for anyio==4.0.0 from https://files.pythonhosted.org/packages/36/55/ad4de788d84a630656ece71059665e01ca793c04294c463fd84132f40fe6/anyio-4.0.0-py3-none-any.whl.metadata\n",
      "  Using cached anyio-4.0.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: async-timeout==4.0.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: attrs==23.1.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: certifi==2023.7.22 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer==3.2.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (3.2.0)\n",
      "Collecting filelock==3.12.3 (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for filelock==3.12.3 from https://files.pythonhosted.org/packages/52/90/45223db4e1df30ff14e8aebf9a1bf0222da2e7b49e53692c968f36817812/filelock-3.12.3-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: frozenlist==1.4.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.4.0)\n",
      "Collecting fsspec==2023.9.0 (from -r requirements.txt (line 10))\n",
      "  Obtaining dependency information for fsspec==2023.9.0 from https://files.pythonhosted.org/packages/3a/9f/b40e8e5be886143379000af5fc0c675352d59e82fd869d24bf784161dc77/fsspec-2023.9.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.9.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting h11==0.14.0 (from -r requirements.txt (line 11))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting httpcore==0.18.0 (from -r requirements.txt (line 12))\n",
      "  Obtaining dependency information for httpcore==0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting httpx==0.25.0 (from -r requirements.txt (line 13))\n",
      "  Obtaining dependency information for httpx==0.25.0 from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub==0.16.4 (from -r requirements.txt (line 14))\n",
      "  Obtaining dependency information for huggingface-hub==0.16.4 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: idna==3.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (3.4)\n",
      "Requirement already satisfied: multidict==6.0.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (6.0.4)\n",
      "Requirement already satisfied: numpy==1.25.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (1.25.2)\n",
      "Requirement already satisfied: openai==0.28.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.28.0)\n",
      "Requirement already satisfied: packaging==23.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (23.1)\n",
      "Requirement already satisfied: pandas==2.1.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2023.3.post1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (2023.3.post1)\n",
      "Collecting PyYAML==6.0.1 (from -r requirements.txt (line 23))\n",
      "  Obtaining dependency information for PyYAML==6.0.1 from https://files.pythonhosted.org/packages/28/09/55f715ddbf95a054b764b547f617e22f1d5e45d83905660e9a088078fe67/PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex==2023.8.8 (from -r requirements.txt (line 24))\n",
      "  Obtaining dependency information for regex==2023.8.8 from https://files.pythonhosted.org/packages/03/5e/9a4cabe86a3b4e67bd2cf795a2e84de01c735c8c1c1d88795425847ccbbe/regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests==2.31.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (2.31.0)\n",
      "Collecting safetensors==0.3.3 (from -r requirements.txt (line 26))\n",
      "  Obtaining dependency information for safetensors==0.3.3 from https://files.pythonhosted.org/packages/34/0e/12d55d5dd648b8f7ea7216c5b7cef9703b4dbd3b2a042872c711d5e98551/safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata\n",
      "  Using cached safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six==1.16.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (1.16.0)\n",
      "Collecting sniffio==1.3.0 (from -r requirements.txt (line 28))\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers==0.13.3 (from -r requirements.txt (line 29))\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Requirement already satisfied: tqdm==4.66.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (4.66.1)\n",
      "Collecting transformers==4.33.1 (from -r requirements.txt (line 31))\n",
      "  Obtaining dependency information for transformers==4.33.1 from https://files.pythonhosted.org/packages/13/30/54b59e73400df3de506ad8630284e9fd63f4b94f735423d55fc342181037/transformers-4.33.1-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting typing_extensions==4.7.1 (from -r requirements.txt (line 32))\n",
      "  Obtaining dependency information for typing_extensions==4.7.1 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: tzdata==2023.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (2023.3)\n",
      "Requirement already satisfied: urllib3==2.0.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (2.0.4)\n",
      "Requirement already satisfied: yarl==1.9.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (1.9.2)\n",
      "Using cached anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "Using cached filelock-3.12.3-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
      "Using cached httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "Using cached httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl (289 kB)\n",
      "Using cached safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
      "Using cached transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: tokenizers, safetensors, typing_extensions, sniffio, regex, PyYAML, h11, fsspec, filelock, huggingface-hub, anyio, transformers, httpcore, httpx\n",
      "Successfully installed PyYAML-6.0.1 anyio-4.0.0 filelock-3.12.3 fsspec-2023.9.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.16.4 regex-2023.8.8 safetensors-0.3.3 sniffio-1.3.0 tokenizers-0.13.3 transformers-4.33.1 typing_extensions-4.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg[binary,pool] in /home/jovyan/resources/venv/lib/python3.9/site-packages (3.1.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /opt/conda/lib/python3.9/site-packages (from psycopg[binary,pool]) (4.2.0)\n",
      "Requirement already satisfied: psycopg-binary==3.1.9 in /home/jovyan/resources/venv/lib/python3.9/site-packages (from psycopg[binary,pool]) (3.1.9)\n",
      "Requirement already satisfied: psycopg-pool in /home/jovyan/resources/venv/lib/python3.9/site-packages (from psycopg[binary,pool]) (3.1.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/jovyan/resources/venv/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"psycopg[binary,pool]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "# from dotenv import dotenv_values\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import asdict, dataclass\n",
    "from datetime import datetime, timezone\n",
    "from itertools import combinations\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import psycopg\n",
    "from pandas import option_context\n",
    "from psycopg.rows import dict_row\n",
    "from collections import defaultdict\n",
    "from openai_info import OPENAI_API_KEY, OPENAI_ORGANIZATION #\n",
    "import httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    json_data = None\n",
    "    try:\n",
    "        with open(json_path, \"r\") as json_fp:\n",
    "            json_data = json.load(json_fp)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def write_json(data: Union[Dict, List], file_name: str) -> str:\n",
    "    out_file_path = os.path.abspath(file_name)\n",
    "    with open(out_file_path, \"w\") as json_fp:\n",
    "        json.dump(data, json_fp)\n",
    "    return out_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks = read_json(\"random_tasks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = OPENAI_ORGANIZATION\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"\n",
    "# You are an AI quality evaluator. Your role involves judging the accuracy of an AI model's response using the provided <source> text. Here are the steps:\n",
    "# 1. Read the <source> text to understand the task's context.\n",
    "# 2. Grasp the given <question>.\n",
    "# 3. Understand the <actual_answer>, the ideal answer to the <question> using only the information from the <source>.\n",
    "# 4. Examine the AI's <answer> to the same <question>.\n",
    "# 5. Critically evaluate the <answer>, considering aspects like accuracy, conciseness, organization, and usefulness in relation to the <source>.\n",
    "# 6. Compare the <answer> to the <actual_answer>, noting similarities and differences.\n",
    "# 7. Assign a score to the <answer> on a scale of 1 to 10, where 1 means no match, and 10 means almost identical to <ground_truth>.\n",
    "# 8. Provide your judgment and reasoning in a JSON format. Include \"correct\" (true if the <answer> aligns closely with the <ground_truth>), \"score\", and \"reason\" (explaining your score with clear evidence from the <source> and <answer>). An example: {\"correct\": true, \"score\": 8, \"reason\": \"The answer is concise, accurate, and follows the source closely.\"}.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate system prompt that assigns a letter grade instead of a value (0-10). There are also 10 possible values here, but maybe the judge might be more intentional about picking a letter vs a number.\n",
    "system_prompt = \"\"\"\n",
    "As an AI quality evaluator, your role is to gauge the accuracy of an AI model's response using the provided <source> text. Follow these steps:\n",
    "\n",
    "1.) Understand the context by reading the <source> text.\n",
    "2.) Comprehend the given <question>.\n",
    "3.) Determine the <actual_answer>â€”the ideal answer using only the <source> information.\n",
    "4.) Review the AI's <answer>.\n",
    "5.) Analyze the <answer>'s accuracy, brevity, structure, and utility in relation to the <source>.\n",
    "6.) Contrast the <answer> and the <actual_answer>.\n",
    "7.) Assign a grade to the <answer> using A+ to F- scale, A+ being nearly identical to <ground_truth> and F- being entirely dissimilar.\n",
    "8.) Document your judgment and reasoning in JSON format: include \"correct\" (true if <answer> aligns closely with <ground_truth>), \"score\", and \"reason\" (justifying your score with clear evidence from <source> and <answer>).\n",
    "Note: Favor responses that closely align with the source and discourage those incorporating outside information. The aim is to evaluate the AI's understanding of the provided source, not its knowledge from training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_llm(\n",
    "    prompt_text: str, llm_endpoint: str, client: httpx.AsyncClient\n",
    "):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"inputs\": f\"<|prompt|>{prompt_text}<|endoftext|><|answer|>\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"min_new_tokens\": 2,\n",
    "            \"num_beams\": 1,\n",
    "            \"do_sample\": False,\n",
    "            \"temperature\": 0.3,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"renormalize_logits\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = await client.post(\n",
    "            llm_endpoint,\n",
    "            json=data,\n",
    "            headers=headers,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    except httpx.RequestError as exc:\n",
    "        print(f\"An error occurred while requesting {exc.request.url!r}.\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(\n",
    "            f\"Error response {exc.response.status_code} while requesting {exc.request.url!r}.\"\n",
    "        )\n",
    "    else:\n",
    "        try:\n",
    "            response_text = response.json()[\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_questions(json_file: str, llm_endpoint: str, client: httpx.AsyncClient):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    answer_data = []\n",
    "    for entry in data:\n",
    "        prompt_text = f\"\"\" {entry['source']} Given the information above and only the information from above, answer the following question {entry['question']} \"\"\"\n",
    "        answer_text = await ask_llm(prompt_text, llm_endpoint, client)\n",
    "\n",
    "        if answer_text:\n",
    "            answer_data.append({'id': entry['id'], 'answer': answer_text})\n",
    "\n",
    "    with open('answers.json', 'w') as file:\n",
    "        json.dump(answer_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def judge_questions(system_prompt, eval_model_name, prompt_json, answer_json):    \n",
    "    with open(prompt_json, 'r') as file:\n",
    "        prompt_data = json.load(file)\n",
    "    \n",
    "    with open(answer_json, 'r') as file:\n",
    "        answer_data = json.load(file)\n",
    "        \n",
    "    eval_data = []\n",
    "    results = []\n",
    "        \n",
    "    for i in range(len(prompt_data)):\n",
    "        prompt_entry = prompt_data[i]\n",
    "        answer_entry = answer_data[i]\n",
    "        template = f\"\"\"\n",
    "        <source>\n",
    "        {prompt_entry['source']}\n",
    "        </source>\n",
    "\n",
    "        <question>\n",
    "        {prompt_entry['question']}\n",
    "        </question>\n",
    "\n",
    "        <actual_answer>\n",
    "        {prompt_entry['answer']}\n",
    "        </actual_answer>\n",
    "\n",
    "        <answer>\n",
    "        {answer_entry['answer']}\n",
    "        </answer>\n",
    "        \"\"\"\n",
    "\n",
    "        chat_completion_resp = await openai.ChatCompletion.acreate(\n",
    "            model=eval_model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": template,\n",
    "                },\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        if chat_completion_resp.choices[0].message.content:\n",
    "            eval_data.append({'id': answer_entry['id'], 'eval_result': chat_completion_resp.choices[0].message.content})\n",
    "            results.append({'id': prompt_entry['id'], 'source': prompt_entry['source'], 'question': prompt_entry['question'], 'ground truth': prompt_entry['answer'], 'answer': answer_entry['answer']})\n",
    "\n",
    "    with open('eval_marco.json', 'w') as file:\n",
    "        json.dump(eval_data, file)\n",
    "    with open('results_marco.json', 'w') as file:\n",
    "        json.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1\"\n",
    "eval_model_name = \"gpt-4-0613\"\n",
    "llm_endpoint = \"https://chat-large.api.h2o.ai/generate\"\n",
    "auth = httpx.BasicAuth(\"user\", \"bhx5xmu6UVX4\")\n",
    "client = httpx.AsyncClient(auth=auth, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "await answer_questions(\"prompts_marco.json\", llm_endpoint, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_name = \"gpt-4-0613\"\n",
    "await judge_questions(system_prompt, eval_model_name, \"prompts_marco.json\", \"answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
